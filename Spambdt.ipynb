{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMcr8mncKnWnLfHavqeb1+y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AryanSathish3/Machinelearning25/blob/main/Spambdt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Open in Colab - corrected and ready to run\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Column names (from spambase)\n",
        "column_names = [\n",
        "    'word_freq_make', 'word_freq_address', 'word_freq_all', 'word_freq_3d',\n",
        "    'word_freq_our', 'word_freq_over', 'word_freq_remove', 'word_freq_internet',\n",
        "    'word_freq_order', 'word_freq_mail', 'word_freq_receive', 'word_freq_will',\n",
        "    'word_freq_people', 'word_freq_report', 'word_freq_addresses', 'word_freq_free',\n",
        "    'word_freq_business', 'word_freq_email', 'word_freq_you', 'word_freq_credit',\n",
        "    'word_freq_your', 'word_freq_font', 'word_freq_000', 'word_freq_money',\n",
        "    'word_freq_hp', 'word_freq_hpl', 'word_freq_george', 'word_freq_650',\n",
        "    'word_freq_lab', 'word_freq_labs', 'word_freq_telnet', 'word_freq_857',\n",
        "    'word_freq_data', 'word_freq_415', 'word_freq_85', 'word_freq_technology',\n",
        "    'word_freq_1999', 'word_freq_parts', 'word_freq_pm', 'word_freq_direct',\n",
        "    'word_freq_cs', 'word_freq_meeting', 'word_freq_original', 'word_freq_project',\n",
        "    'word_freq_re', 'word_freq_edu', 'word_freq_table', 'word_freq_conference',\n",
        "    'char_freq_semicolon', 'char_freq_parenthesis', 'char_freq_bracket', 'char_freq_exclamation',\n",
        "    'char_freq_dollar', 'char_freq_hash', 'capital_run_length_average',\n",
        "    'capital_run_length_longest', 'capital_run_length_total', 'spam'\n",
        "]\n",
        "\n",
        "# Load dataset (update path if necessary)\n",
        "df = pd.read_csv('/content/spambase.data', header=None, names=column_names)\n",
        "print(\"Loaded dataset with shape:\", df.shape)\n",
        "print(df.head())\n",
        "\n",
        "# Features and target\n",
        "X = df.drop(columns='spam')\n",
        "y = df['spam']\n",
        "\n",
        "# Train-test split (stratified)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "print(\"Train shape:\", X_train.shape, \"Test shape:\", X_test.shape)\n",
        "\n",
        "# -------------------------\n",
        "# Gaussian Naive Bayes-like Bayesian Classifier\n",
        "# -------------------------\n",
        "class BayesianClassifier:\n",
        "    def __init__(self, eps_std=1e-6, eps_likelihood=1e-12):\n",
        "        self.priors = {}            # prior probability per class\n",
        "        self.feature_params = {}    # {class: {feature: {'mean':..., 'std':...}}}\n",
        "        self.eps_std = eps_std\n",
        "        self.eps_likelihood = eps_likelihood\n",
        "\n",
        "    def fit(self, X_train: pd.DataFrame, y_train: pd.Series):\n",
        "        classes = np.unique(y_train)\n",
        "        for c in classes:\n",
        "            # prior: proportion of samples in class c\n",
        "            mask = (y_train.values == c)\n",
        "            X_c = X_train.iloc[mask]\n",
        "            self.priors[int(c)] = X_c.shape[0] / X_train.shape[0]\n",
        "\n",
        "            # compute mean and std (use ddof=0 for population std)\n",
        "            params = {}\n",
        "            for col in X_train.columns:\n",
        "                col_vals = X_c[col].values\n",
        "                mean = np.mean(col_vals) if col_vals.size > 0 else 0.0\n",
        "                std = np.std(col_vals, ddof=0) if col_vals.size > 0 else 0.0\n",
        "                if std <= self.eps_std:\n",
        "                    std = self.eps_std\n",
        "                params[col] = {'mean': float(mean), 'std': float(std)}\n",
        "            self.feature_params[int(c)] = params\n",
        "\n",
        "    def calculate_likelihood(self, x_value: float, feature_name: str, class_label: int) -> float:\n",
        "        p = self.feature_params[class_label][feature_name]\n",
        "        mean, std = p['mean'], p['std']\n",
        "        likelihood = norm.pdf(x_value, loc=mean, scale=std)\n",
        "        # clip to avoid exact zero\n",
        "        return float(np.clip(likelihood, self.eps_likelihood, None))\n",
        "\n",
        "    def calculate_posterior(self, x_row: pd.Series) -> dict:\n",
        "        log_post = {}\n",
        "        for c, prior in self.priors.items():\n",
        "            # if prior is 0 (shouldn't be with stratify) skip\n",
        "            if prior <= 0:\n",
        "                log_post[c] = -np.inf\n",
        "                continue\n",
        "            lp = np.log(prior)\n",
        "            for feature_name, value in x_row.items():\n",
        "                likelihood = self.calculate_likelihood(value, feature_name, c)\n",
        "                lp += np.log(likelihood)\n",
        "            log_post[c] = lp\n",
        "\n",
        "        # convert log-posteriors to normalized probabilities (stable softmax)\n",
        "        max_lp = max(log_post.values())\n",
        "        exps = {c: np.exp(lp - max_lp) for c, lp in log_post.items()}\n",
        "        s = sum(exps.values())\n",
        "        probs = {c: exps[c] / s for c in exps}\n",
        "        return probs\n",
        "\n",
        "    def predict(self, X_test: pd.DataFrame) -> np.ndarray:\n",
        "        preds = []\n",
        "        # iterate rows (acceptable for this dataset; vectorization is possible but more code)\n",
        "        for _, row in X_test.iterrows():\n",
        "            probs = self.calculate_posterior(row)\n",
        "            pred = max(probs, key=probs.get)\n",
        "            preds.append(int(pred))\n",
        "        return np.array(preds, dtype=int)\n",
        "\n",
        "\n",
        "# Train and evaluate\n",
        "model = BayesianClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Model trained.\")\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {acc:.4f}\")\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\\n\", cm)\n",
        "\n",
        "cr = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\\n\", cr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ph7M8Puxl-yN",
        "outputId": "dd39f4bd-6be9-4aff-d2cd-f79c8e86a8f4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded dataset with shape: (4601, 58)\n",
            "   word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
            "0            0.00               0.64           0.64           0.0   \n",
            "1            0.21               0.28           0.50           0.0   \n",
            "2            0.06               0.00           0.71           0.0   \n",
            "3            0.00               0.00           0.00           0.0   \n",
            "4            0.00               0.00           0.00           0.0   \n",
            "\n",
            "   word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
            "0           0.32            0.00              0.00                0.00   \n",
            "1           0.14            0.28              0.21                0.07   \n",
            "2           1.23            0.19              0.19                0.12   \n",
            "3           0.63            0.00              0.31                0.63   \n",
            "4           0.63            0.00              0.31                0.63   \n",
            "\n",
            "   word_freq_order  word_freq_mail  ...  char_freq_semicolon  \\\n",
            "0             0.00            0.00  ...                 0.00   \n",
            "1             0.00            0.94  ...                 0.00   \n",
            "2             0.64            0.25  ...                 0.01   \n",
            "3             0.31            0.63  ...                 0.00   \n",
            "4             0.31            0.63  ...                 0.00   \n",
            "\n",
            "   char_freq_parenthesis  char_freq_bracket  char_freq_exclamation  \\\n",
            "0                  0.000                0.0                  0.778   \n",
            "1                  0.132                0.0                  0.372   \n",
            "2                  0.143                0.0                  0.276   \n",
            "3                  0.137                0.0                  0.137   \n",
            "4                  0.135                0.0                  0.135   \n",
            "\n",
            "   char_freq_dollar  char_freq_hash  capital_run_length_average  \\\n",
            "0             0.000           0.000                       3.756   \n",
            "1             0.180           0.048                       5.114   \n",
            "2             0.184           0.010                       9.821   \n",
            "3             0.000           0.000                       3.537   \n",
            "4             0.000           0.000                       3.537   \n",
            "\n",
            "   capital_run_length_longest  capital_run_length_total  spam  \n",
            "0                          61                       278     1  \n",
            "1                         101                      1028     1  \n",
            "2                         485                      2259     1  \n",
            "3                          40                       191     1  \n",
            "4                          40                       191     1  \n",
            "\n",
            "[5 rows x 58 columns]\n",
            "Train shape: (3680, 57) Test shape: (921, 57)\n",
            "Model trained.\n",
            "Accuracy: 0.6840\n",
            "Confusion Matrix:\n",
            " [[275 283]\n",
            " [  8 355]]\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.49      0.65       558\n",
            "           1       0.56      0.98      0.71       363\n",
            "\n",
            "    accuracy                           0.68       921\n",
            "   macro avg       0.76      0.74      0.68       921\n",
            "weighted avg       0.81      0.68      0.68       921\n",
            "\n"
          ]
        }
      ]
    }
  ]
}